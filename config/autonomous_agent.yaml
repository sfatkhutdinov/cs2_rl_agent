# Autonomous Agent Configuration for Cities: Skylines 2
# This configuration maximizes exploration and learning without human guidance

experiment_name: "fully_autonomous_cs2_agent"
seed: 42  # Random seed for reproducibility

# Interface settings
interface:
  type: "auto_vision"  # Use the automatic vision interface
  vision:
    detection_method: "ocr"  # Use OCR for text detection
    cache_detections: true  # Cache detected UI elements for faster access
    ocr_confidence: 0.5  # Lower threshold to detect more potential UI elements
    template_threshold: 0.6  # Lower threshold for more detection possibilities
    screen_region: [0, 0, 1920, 1080]  # Default full screen region (will be adjusted automatically)

# Environment settings
environment:
  type: "cs2"  # Environment type identifier
  observation_space:
    type: "combined"  # Use both visual and metric observations
    include_visual: true  # Enable visual observations
    image_size: [84, 84]  # Observation resolution
    grayscale: true  # Convert to grayscale (renamed from use_grayscale)
    normalize_metrics: true  # Normalize metric values
    metrics:  # List of metrics to track
      - "population"
      - "happiness"
      - "unemployment"
      - "education"
      - "health"
      - "pollution"
      - "traffic"
      - "budget_balance"
      - "tax_income"
      - "land_value"
  
  action_space:
    type: "advanced"  # Use the advanced action space
    continuous: false  # Use discrete actions
    zone:  # Zoning action types
      - "residential"
      - "commercial"
      - "industrial"
      - "office"
      - "delete_zone"
    infrastructure:  # Infrastructure action types
      - "road"
      - "power_line"
      - "water_pipe"
      - "park"
      - "service_building"
      - "delete_infrastructure"
    budget:  # Budget adjustment actions
      - "increase_residential_tax"
      - "decrease_residential_tax"
      - "increase_commercial_tax"
      - "decrease_commercial_tax"
      - "increase_industrial_tax"
      - "decrease_industrial_tax"
      - "increase_service_budget"
      - "decrease_service_budget"
    
  reward_function:
    type: "balanced"  # Use a balanced reward function
    weights:
      population: 0.3
      happiness: 0.2
      budget: 0.2
      traffic: 0.2
      discovery: 0.1  # Reward for discovering new UI elements
  
  max_episode_steps: 2000  # Allow longer episodes for proper exploration
  metrics_update_freq: 10   # Update metrics frequently
  pause_on_menu: false     # Don't pause game during menu navigation
  
  # Metrics to track
  metrics:
    - "population"
    - "happiness"
    - "unemployment"
    - "education"
    - "health"
    - "pollution"
    - "traffic"
    - "budget_balance"
    - "tax_income"
    - "land_value"

# Exploration settings
exploration:
  frequency: 0.4                  # Higher exploration frequency (40% of actions)
  random_action_frequency: 0.3    # Higher random action frequency (30% of exploration actions)
  menu_buffer_size: 100           # Store more discovered menu items
  intrinsic_reward_coef: 0.2      # Coefficient for intrinsic reward (curiosity-driven exploration)
  discovery_bonus: 0.1            # Reward bonus for discovering new UI elements
  
  # Visual exploration regions to prioritize
  priority_regions:
    - "top_menu"
    - "left_sidebar"
    - "bottom_bar"
    - "dialogs"

# Agent settings (PPO algorithm)
agent:
  policy_type: "MlpLstmPolicy"    # Use LSTM for sequential decision making
  learning_rate: 0.0003
  n_steps: 2048                   # Steps per update
  batch_size: 64
  n_epochs: 10
  gamma: 0.99                     # Discount factor
  gae_lambda: 0.95                # GAE lambda parameter
  clip_range: 0.2
  normalize_advantage: true
  ent_coef: 0.02                  # Higher entropy coefficient for more exploration
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: true                   # Use state-dependent exploration
  sde_sample_freq: 4
  
  # Policy network architecture (using LSTM for temporal memory)
  policy_kwargs:
    net_arch:
      - {"pi": [128, 128], "vf": [128, 128]}  # Shared network
      - {"lstm": 128}                          # LSTM layer
      - {"pi": [64], "vf": [64]}              # Policy and value heads

# Training settings
training:
  total_timesteps: 10000000       # Train for 10 million steps
  n_envs: 1                       # Only use 1 environment (game instance)
  checkpoint_freq: 50000          # Save checkpoint every 50k steps
  evaluate_during_training: true
  eval_freq: 100000               # Evaluate every 100k steps
  eval_episodes: 3                # Number of episodes for evaluation
  
  # Progress tracking
  log_interval: 10
  verbose: 1

# Paths for saving data
paths:
  log_dir: "logs/autonomous"
  model_dir: "models/autonomous"
  tensorboard_dir: "tensorboard/autonomous"
  data_dir: "data/autonomous"

# Curriculum learning (gradually increasing difficulty)
curriculum:
  enabled: true
  levels:
    - name: "initial_exploration"
      steps: 1000000
      exploration_frequency: 0.6
      reward_weights:
        discovery: 0.6
        city_metrics: 0.4
    
    - name: "balanced_learning"
      steps: 3000000
      exploration_frequency: 0.4
      reward_weights:
        discovery: 0.4
        city_metrics: 0.6
    
    - name: "focused_optimization"
      steps: 6000000
      exploration_frequency: 0.2
      reward_weights:
        discovery: 0.1
        city_metrics: 0.9 