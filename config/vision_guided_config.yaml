# Vision-Guided Autonomous Agent Configuration for Cities: Skylines 2 RL
# Optimized for utilizing the Ollama vision model (granite3.2-vision:latest)
experiment_name: "vision_guided_cs2_agent"
seed: 42

# Environment configuration
environment:
  # Interface config
  interface:
    type: "ollama_vision"  # Using the Ollama vision interface
    templates_dir: "templates"
    ocr_enabled: true
    vision:  # Added vision configuration
      detection_method: "ollama"
      ocr_confidence: 0.7
      template_threshold: 0.8
      cache_detections: true
      screen_capture_fps: 10
      screen_region: [0, 0, 1920, 1080]  # Full HD resolution
  
  # Ollama vision model configuration
  ollama:
    url: "http://localhost:11434/api/generate"
    model: "granite3.2-vision:latest"
    max_tokens: 300  # Further reduced to avoid extra text
    temperature: 0.01  # Almost deterministic responses
    cache_ttl: 30  # Cache responses for 30 seconds
    guidance_update_freq: 100  # Update vision guidance every 100 steps
    screenshot_frequency: 10  # Save every 10th screenshot
    response_timeout: 10  # Reduced timeout for faster training
    retries: 2  # Reduced retries
    retry_delay: 0.5  # Shorter retry delay
    json_validation: true  # Enable strict JSON validation
  
  # Observation space configuration - enhanced for better visual processing
  observation_space:
    type: "combined"
    include_visual: true
    include_metrics: true
    image_size: [128, 128]  # Increased from 84x84 to capture more details
    grayscale: false  # Use color for better feature detection
    normalize_metrics: true
    metrics: ["population", "happiness", "budget_balance", "traffic"]
    
  # Action space configuration
  action_space:
    type: "discrete"
    zone:
      - "residential"
      - "commercial" 
      - "industrial"
    infrastructure:
      - "road"
      - "power_line"
      - "water_pipe"
      - "power_plant"
      - "water_pump"
      - "wind_turbine"
      - "bulldoze"
    budget:
      - "increase_tax"
      - "decrease_tax"
      - "increase_budget"
      - "decrease_budget"
      - "pause"
      - "increase_speed"
      - "decrease_speed"
      - "camera_up"
      - "camera_down"
      - "camera_left"
      - "camera_right"
      - "camera_zoom_in"
      - "camera_zoom_out"
      - "open_menu"
      - "close_menu"
  
  # Reward function configuration - more nuanced rewards with focus on population
  reward_function:
    population_growth: 0.2  # Increased weight on population growth
    happiness: 0.05
    budget_balance: 0.1
    traffic_flow: 0.1
    bankruptcy_penalty: 1.0
    pollution_penalty: 0.05
    
  # Environment parameters
  max_episode_steps: 3000  # Longer episodes for better learning
  metrics_update_freq: 5   # More frequent updates
  pause_on_menu: false
  action_repeat: 1         # Can be increased for frame skipping

# Agent configuration - optimized for high-end hardware
agent:
  policy_type: "MultiInputPolicy"  # Required for dictionary observation spaces
  policy_kwargs:
    net_arch:
      pi: [256, 128, 64]   # Deeper and wider policy network
      vf: [256, 128, 64]   # Deeper and wider value network
    activation_fn: "relu"  # Will be mapped to torch.nn.ReLU in code
    ortho_init: true
    normalize_images: true
    optimizer_kwargs:
      eps: 0.00001
      weight_decay: 0.00001
  
  # Training hyperparameters optimized for faster learning
  learning_rate: 0.0003
  n_steps: 2048            # Larger horizons for better credit assignment
  batch_size: 512          # Larger batches to utilize GPU
  n_epochs: 10             # More epochs per update
  gamma: 0.99
  gae_lambda: 0.95
  ent_coef: 0.01           # Slightly higher exploration
  vf_coef: 1.0             # Better value function estimation
  max_grad_norm: 0.5
  target_kl: 0.02          # More optimization steps per update
  
# Training configuration
training:
  n_envs: 1                # Must be 1 for single-game interaction
  total_timesteps: 20000000  # Double training steps for more learning
  checkpoint_freq: 50000
  evaluate_during_training: true
  eval_freq: 100000
  eval_episodes: 3
  
# Autonomous exploration config
autonomous:
  exploration_frequency: 0.3  # Slightly reduced in favor of vision guidance
  random_action_frequency: 0.1  # Reduced in favor of more structured exploration
  menu_exploration_buffer_size: 100  # Larger buffer to explore more menus
  
# Vision guidance config
vision_guidance:
  enabled: true
  frequency: 0.1  # 10% chance to use vision guidance
  cache_ttl: 30  # Cache vision guidance for 30 seconds
  min_confidence: 0.7  # Minimum confidence for vision-guided actions
  max_consecutive_attempts: 3  # Max consecutive vision-guided actions
  background_analysis: true  # Run vision analysis in background
  
# Memory monitoring config - increased for high-end hardware
memory_monitor:
  enabled: true
  memory_limit_gb: 32      # Increased from 12GB to 32GB (half of 64GB)
  disk_limit_gb: 200       # Increased disk usage allowance
  check_interval: 10000 